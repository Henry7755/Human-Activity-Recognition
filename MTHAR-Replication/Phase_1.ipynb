{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3d752e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "def generate_windows(feature_length, base_length, scales):\n",
    "    \"\"\"\n",
    "    Generate temporal anchor windows centered on each feature position.\n",
    "\n",
    "    Args:\n",
    "        feature_length (int): Length of backbone feature sequence (T)\n",
    "        base_length (float): Base window length (n)\n",
    "        scales (list): List of scale factors (e.g., [1, 2, 4])\n",
    "\n",
    "    Returns:\n",
    "        windows (np.ndarray): shape (num_windows, 2)\n",
    "                              each row = [center, length]\n",
    "    \"\"\"\n",
    "    windows = []\n",
    "\n",
    "    for center in range(feature_length):\n",
    "        for s in scales:\n",
    "            l1 = base_length * np.sqrt(s)\n",
    "            l2 = base_length / np.sqrt(s)\n",
    "\n",
    "            windows.append([center, l1])\n",
    "            windows.append([center, l2])\n",
    "\n",
    "    return np.array(windows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffaac3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy = tf.random.normal((1, input_length, C_in))\n",
    "out = backbone(dummy)\n",
    "print(out.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebbf55b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-18 15:34:06.294487: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2026-02-18 15:34:18.088076: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2026-02-18 15:34:39.413079: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Preparing data..\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/gaowenbing/desktop/dd/Torch_Har_cbam/HAR_Dataset/uci_har/np_train_x.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 18\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m==> Preparing data..\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     16\u001b[0m DATA_DIR \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/home/gaowenbing/desktop/dd/Torch_Har_cbam/HAR_Dataset/uci_har/\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 18\u001b[0m train_x \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDATA_DIR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnp_train_x.npy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m     19\u001b[0m train_x \u001b[38;5;241m=\u001b[39m train_x\u001b[38;5;241m.\u001b[39mreshape(train_x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], train_x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], train_x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m], \u001b[38;5;241m1\u001b[39m)   \u001b[38;5;66;03m# (N, H, W, C)  – channels-last\u001b[39;00m\n\u001b[1;32m     21\u001b[0m train_y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(DATA_DIR, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnp_train_y.npy\u001b[39m\u001b[38;5;124m'\u001b[39m))\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32)\n",
      "File \u001b[0;32m~/miniconda3/envs/ai_research/lib/python3.10/site-packages/numpy/lib/_npyio_impl.py:451\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    449\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 451\u001b[0m     fid \u001b[38;5;241m=\u001b[39m stack\u001b[38;5;241m.\u001b[39menter_context(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    452\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    454\u001b[0m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/gaowenbing/desktop/dd/Torch_Har_cbam/HAR_Dataset/uci_har/np_train_x.npy'"
     ]
    }
   ],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "\n",
    "\n",
    "# ──────────────────────────────────────────────\n",
    "# 2. Data loading\n",
    "# ──────────────────────────────────────────────\n",
    "print('==> Preparing data..')\n",
    "\n",
    "DATA_DIR = '/home/gaowenbing/desktop/dd/Torch_Har_cbam/HAR_Dataset/uci_har/'\n",
    "\n",
    "train_x = np.load(os.path.join(DATA_DIR, 'np_train_x.npy')).astype(np.float32)\n",
    "train_x = train_x.reshape(train_x.shape[0], train_x.shape[1], train_x.shape[2], 1)   # (N, H, W, C)  – channels-last\n",
    "\n",
    "train_y = np.load(os.path.join(DATA_DIR, 'np_train_y.npy')).astype(np.float32)\n",
    "\n",
    "test_x = np.load(os.path.join(DATA_DIR, 'np_test_x.npy')).astype(np.float32)\n",
    "test_x = test_x.reshape(test_x.shape[0], test_x.shape[1], test_x.shape[2], 1)\n",
    "\n",
    "test_y = np.load(os.path.join(DATA_DIR, 'np_test_y.npy')).astype(np.float32)\n",
    "\n",
    "print('train_x:', train_x.shape, '  train_y:', train_y.shape)\n",
    "print('test_x: ', test_x.shape,  '  test_y: ', test_y.shape)\n",
    "\n",
    "# tf.data pipelines\n",
    "BATCH_SIZE = 256\n",
    "TEST_BATCH  = 2947\n",
    "\n",
    "train_dataset = (\n",
    "    tf.data.Dataset.from_tensor_slices((train_x, train_y))\n",
    "    .shuffle(buffer_size=10000)\n",
    "    .batch(BATCH_SIZE)\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")\n",
    "\n",
    "test_dataset = (\n",
    "    tf.data.Dataset.from_tensor_slices((test_x, test_y))\n",
    "    .batch(TEST_BATCH)\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")\n",
    "\n",
    "# ──────────────────────────────────────────────\n",
    "# 3. SKConv layer\n",
    "#    PyTorch layout: (N, C, H, W)  →  TF layout: (N, H, W, C)\n",
    "#    PyTorch kernel=(3,1), padding=(1+i, 1), dilation=(1+i, 1)\n",
    "#    → TF Conv2D with kernel_size=(3,1), padding='same' per branch\n",
    "# ──────────────────────────────────────────────\n",
    "\n",
    "class SKConv(layers.Layer):\n",
    "    \"\"\"Selective-Kernel Convolution block (TensorFlow/Keras, channels-last).\"\"\"\n",
    "\n",
    "    def __init__(self, features, M=3, G=32, r=32, stride=1, L=32, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        d = max(int(features / r), L)\n",
    "        self.M = M\n",
    "        self.features = features\n",
    "\n",
    "        # M parallel branches with different dilation rates\n",
    "        self.branches = []\n",
    "        for i in range(M):\n",
    "            branch = keras.Sequential([\n",
    "                # groups → use DepthwiseConv2D trick or just Conv2D\n",
    "                # PyTorch groups=G  ≈  TF Conv2D with groups=G (TF 2.x supports it)\n",
    "                layers.Conv2D(99999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999\n",
    "                    filters=features,\n",
    "                    kernel_size=(3, 1),\n",
    "                    strides=(stride, 1),\n",
    "                    padding='same',\n",
    "                    dilation_rate=(1 + i, 1),\n",
    "                    groups=G,\n",
    "                    use_bias=False,\n",
    "                ),\n",
    "                layers.BatchNormalization(),\n",
    "                layers.ReLU(),\n",
    "            ], name=f'branch_{i}')\n",
    "            self.branches.append(branch)\n",
    "\n",
    "        # Global Average Pool + FC (implemented as Conv2D(1,1) to keep 4-D tensor)\n",
    "        self.gap = layers.GlobalAveragePooling2D(keepdims=True)   # → (N, 1, 1, C)\n",
    "        self.fc = keras.Sequential([\n",
    "            layers.Conv2D(d, kernel_size=1, use_bias=False),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.ReLU(),\n",
    "        ], name='fc_z')\n",
    "\n",
    "        # One 1×1 conv per branch to produce attention logits\n",
    "        self.fcs = [\n",
    "            layers.Conv2D(features, kernel_size=1, name=f'fc_attn_{i}')\n",
    "            for i in range(M)\n",
    "        ]\n",
    "        self.softmax = layers.Softmax(axis=1)   # softmax across branch dimension\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        batch_size = tf.shape(x)[0]\n",
    "\n",
    "        # Each branch output: (N, H, W, features)\n",
    "        branch_feats = [branch(x, training=training) for branch in self.branches]\n",
    "\n",
    "        # Stack → (N, M, H, W, features)\n",
    "        feats = tf.stack(branch_feats, axis=1)\n",
    "\n",
    "        # Fuse: element-wise sum across branches → (N, H, W, features)\n",
    "        feats_U = tf.reduce_sum(feats, axis=1)\n",
    "\n",
    "        # Channel descriptor via GAP → (N, 1, 1, features)\n",
    "        feats_S = self.gap(feats_U)\n",
    "\n",
    "        # Compact feature → (N, 1, 1, d)\n",
    "        feats_Z = self.fc(feats_S, training=training)\n",
    "\n",
    "        # Per-branch attention vectors → list of (N, 1, 1, features)\n",
    "        attn_vectors = [fc(feats_Z) for fc in self.fcs]\n",
    "\n",
    "        # Stack → (N, M, 1, 1, features), then softmax over branch dim\n",
    "        attn_vectors = tf.stack(attn_vectors, axis=1)\n",
    "        attn_vectors = self.softmax(attn_vectors)           # (N, M, 1, 1, features)\n",
    "\n",
    "        # Weighted sum: (N, M, H, W, features) * (N, M, 1, 1, features)\n",
    "        feats_V = tf.reduce_sum(feats * attn_vectors, axis=1)  # (N, H, W, features)\n",
    "\n",
    "        return feats_V\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update(dict(features=self.features, M=self.M))\n",
    "        return config\n",
    "\n",
    "\n",
    "# ──────────────────────────────────────────────\n",
    "# 4. SKNet model\n",
    "# ──────────────────────────────────────────────\n",
    "\n",
    "def build_sknet(M=3, G=32, r=32, stride=1, L=32, num_classes=6):\n",
    "    \"\"\"Returns a compiled Keras Model equivalent to the PyTorch SKNet.\"\"\"\n",
    "    inputs = keras.Input(shape=(None, None, 1), name='input')   # (H, W, 1)\n",
    "\n",
    "    # conv1: PyTorch Conv2d(1, 64, (5,1), stride=(3,1), padding=(1,0))\n",
    "    # channels-last TF equivalent:\n",
    "    x = layers.Conv2D(64, kernel_size=(5, 1), strides=(3, 1), padding='same', use_bias=True)(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "\n",
    "    # NOTE: PyTorch conv2_sk expects 128 input features but conv1 outputs 64.\n",
    "    # The original code passes 64-channel output to SKConv(128, ...) which would\n",
    "    # error in PyTorch too unless conv1 produces 128. We mirror the code as-is\n",
    "    # and set SKConv to match actual channel count (64 → 64, then 64 → 128).\n",
    "    # Adjust these numbers to match your actual checkpoint / intent.\n",
    "    x = SKConv(64,  M=M, G=G, r=r, stride=stride, L=L, name='skconv1')(x)\n",
    "    x = SKConv(128, M=M, G=G, r=r, stride=stride, L=L, name='skconv2')(x)\n",
    "\n",
    "    # Flatten\n",
    "    x = layers.Flatten()(x)\n",
    "\n",
    "    # FC: 6-class output\n",
    "    # PyTorch uses nn.LayerNorm on the logits; replicate with LayerNormalization\n",
    "    x = layers.Dense(num_classes, name='fc')(x)\n",
    "    x = layers.LayerNormalization(name='layer_norm')(x)\n",
    "\n",
    "    model = keras.Model(inputs, x, name='SKNet')\n",
    "    return model\n",
    "\n",
    "\n",
    "# ──────────────────────────────────────────────\n",
    "# 5. Compile\n",
    "# ──────────────────────────────────────────────\n",
    "print('==> Building model..')\n",
    "\n",
    "model = build_sknet()\n",
    "model.summary()\n",
    "\n",
    "WD = 1e-4\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.001, weight_decay=WD)\n",
    "\n",
    "loss_fn = keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "train_x: (7352, 128, 9)   train_y: (7352,)\n",
    "test_x:  (2947, 128, 9)   test_y:  (2947,)\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=loss_fn,\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "\n",
    "# Learning-rate scheduler: StepLR(step_size=50, gamma=0.1)\n",
    "# In Keras this is done via a LearningRateScheduler callback\n",
    "def step_lr_schedule(epoch, lr):\n",
    "    \"\"\"Halve the LR by ×0.1 every 50 epochs.\"\"\"\n",
    "    if epoch > 0 and epoch % 50 == 0:\n",
    "        return lr * 0.1\n",
    "    return lr\n",
    "\n",
    "lr_callback = keras.callbacks.LearningRateScheduler(step_lr_schedule, verbose=1)\n",
    "\n",
    "# ──────────────────────────────────────────────\n",
    "# 6. Training loop\n",
    "# ──────────────────────────────────────────────\n",
    "EPOCHS = 500\n",
    "\n",
    "start_epoch = 0let\n",
    "\n",
    "epoch_list  = []\n",
    "error_list  = []\n",
    "\n",
    "# Custom training loop to replicate per-epoch logging\n",
    "for epoch in range(start_epoch, start_epoch + EPOCHS):\n",
    "    print(f'\\nEpoch: {epoch}')\n",
    "\n",
    "    # --- Train ---\n",
    "    train_results = model.fit(\n",
    "        train_dataset,\n",
    "        epochs=1,\n",
    "        verbose=1,\n",
    "        callbacks=[lr_callback],\n",
    "    )\n",
    "\n",
    "    # --- Test ---\n",
    "    test_results = model.evaluate(test_dataset, verbose=0)\n",
    "    test_loss    = test_results[0]\n",
    "    test_acc     = test_results[1]\n",
    "    test_error   = 1.0 - test_acc\n",
    "\n",
    "    print(f'test: {test_acc:.4f} || {test_error:.4f}')\n",
    "\n",
    "    epoch_list.append(epoch)\n",
    "    error_list.append(test_error)\n",
    "\n",
    "    if test_acc > best_acc:\n",
    "        best_acc = test_acc\n",
    "        model.save_weights('best_sknet_weights.h5')\n",
    "        print(f'  >> New best accuracy: {best_acc:.4f}  (weights saved)')\n",
    "\n",
    "        print(f'\\nTraining complete. Best test accuracy: {best_acc:.4f}')\n",
    "\n",
    "# ──────────────────────────────────────────────\n",
    "# 7. Model statistics  (replaces torchstat)\n",
    "#    Print param count and a summary for input (128, 9, 1).\n",
    "# ──────────────────────────────────────────────\n",
    "stat_model = build_sknet()\n",
    "stat_model.build(input_shape=(None, 128, 9, 1))\n",
    "stat_model.summary()\n",
    "\n",
    "total_params     = stat_model.count_params()\n",
    "trainable_params = sum(\n",
    "    tf.size(w).numpy() for w in stat_model.trainable_weights\n",
    ")\n",
    "print(f'Total parameters:     {total_params:,}')\n",
    "print(f'Trainable parameters: {trainable_params:,}')\n",
    "\n",
    "tf.reduce_mean(t, axis=2, name='reduce_sensors')(x)\n",
    "    # x shape: (B, T_feat, 256)\n",
    "\n",
    "    # ── Recognition & Segmentation heads (Conv1D, Table I) ───────────────────\n",
    "    # Class head: predicts (num_classes+1) logits per anchor per time step\n",
    "cls = layers.Conv1D(n_anchors_per_pos * (num_classes + 1),\n",
    "                        kernel_size=3, padding='same',\n",
    "                        name='cls_conv')(x)         # (B, T_feat, A*(K+1))\n",
    "    # Loc head: predicts 2 offsets per anchor per time step\n",
    "loc = layers.Conv1D(n_anchors_per_pos * 2,\n",
    "                        kernel_size=3, padding='same',\n",
    "                        name='loc_conv')(x)         # (B, T_feat, A*2)\n",
    "\n",
    "    # Reshape to (B, total_anchors, K+1) and (B, total_anchors, 2)\n",
    "cls_out = layers.Reshape((-1, num_classes + 1), name='cls_out')(cls)\n",
    "loc_out = layers.Reshape((-1, 2),               name='loc_out')(loc) \n",
    "    # cls_out shape: (B, total_anchors, K+1)\n",
    "    # loc_out shape: (B, total_anchors, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea27c272",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ──────────────────────────────────────────────\n",
    "# 1. GPU configuration\n",
    "# ──────────────────────────────────────────────\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "best_acc = 0.0\n",
    "start_epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c8cc59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 5.  MULTI-TASK LOSS  (Section III-E, equations 5-8)\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "def smooth_l1(x):\n",
    "    \"\"\"Smooth-L1 / Huber loss (eq. 6).\"\"\"\n",
    "    return tf.where(tf.abs(x) < 1.0, 0.5 * tf.square(x), tf.abs(x) - 0.5)\n",
    "\n",
    "\n",
    "def mthars_loss(cls_pred, loc_pred, cls_true, loc_true, pos_mask,\n",
    "                alpha=ALPHA, beta=BETA, neg_pos_ratio=NEG_POS_RATIO):\n",
    "    \"\"\"\n",
    "    Combined multi-task loss (eq. 8).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    cls_pred  : (B, N, K+1)  – raw logits\n",
    "    loc_pred  : (B, N, 2)\n",
    "    cls_true  : (B, N)       – int class labels (0 = background)\n",
    "    loc_true  : (B, N, 2)\n",
    "    pos_mask  : (B, N)       – bool, True for positive anchors\n",
    "    \"\"\"\n",
    "    batch = tf.shape(cls_pred)[0]\n",
    "    N     = tf.shape(cls_pred)[1]\n",
    "\n",
    "    # ── Localisation loss (positive anchors only) ─────────────────────────────\n",
    "    loc_diff  = loc_pred - loc_true                             # (B, N, 2)\n",
    "    loc_loss_ = tf.reduce_sum(smooth_l1(loc_diff), axis=-1)    # (B, N)\n",
    "    pos_float = tf.cast(pos_mask, tf.float32)\n",
    "    n_pos     = tf.maximum(tf.reduce_sum(pos_float), 1.0)\n",
    "    loc_loss  = tf.reduce_sum(loc_loss_ * pos_float) / n_pos\n",
    "\n",
    "    # ── Classification loss (hard-negative mining) ────────────────────────────\n",
    "    cls_true_oh  = tf.one_hot(cls_true, tf.shape(cls_pred)[-1])\n",
    "    xentropy     = tf.nn.softmax_cross_entropy_with_logits(\n",
    "                       labels=cls_true_oh, logits=cls_pred)     # (B, N)\n",
    "\n",
    "    # Positive loss\n",
    "    pos_cls_loss = tf.reduce_sum(xentropy * pos_float) / n_pos\n",
    "\n",
    "    # Hard-negative mining: pick top neg_pos_ratio * n_pos negatives\n",
    "    neg_mask     = tf.logical_not(pos_mask)\n",
    "    neg_float    = tf.cast(neg_mask, tf.float32)\n",
    "    neg_loss_all = xentropy * neg_float                        # zero out positives\n",
    "\n",
    "    # Sort negatives by descending loss for each batch item\n",
    "    n_neg_keep   = tf.cast(n_pos * neg_pos_ratio, tf.int32)\n",
    "    n_neg_keep   = tf.minimum(n_neg_keep, tf.reduce_sum(tf.cast(neg_mask, tf.int32)))\n",
    "\n",
    "    neg_loss_flat   = tf.reshape(neg_loss_all, [-1])           # (B*N,)\n",
    "    _, top_idx      = tf.math.top_k(neg_loss_flat, k=n_neg_keep)\n",
    "    hard_neg_mask   = tf.zeros_like(neg_loss_flat)\n",
    "    hard_neg_mask   = tf.tensor_scatter_nd_update(\n",
    "                          hard_neg_mask,\n",
    "                          tf.expand_dims(top_idx, 1),\n",
    "                          tf.ones(n_neg_keep, dtype=tf.float32))\n",
    "    hard_neg_mask   = tf.reshape(hard_neg_mask, tf.shape(neg_loss_all))\n",
    "    neg_cls_loss    = tf.reduce_sum(xentropy * hard_neg_mask) / tf.cast(n_neg_keep + 1, tf.float32)\n",
    "\n",
    "    conf_loss = pos_cls_loss + neg_cls_loss\n",
    "    total     = (alpha * conf_loss + beta * loc_loss) / tf.cast(batch, tf.float32)\n",
    "    return total, conf_loss, loc_loss\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6066136c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 6.  NON-MAXIMUM SUPPRESSION  (Section III-B)\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "def nms_1d(windows, scores, iou_threshold=0.4):\n",
    "    \"\"\"\n",
    "    1-D NMS.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    windows : (n, 2)  [centre, length]\n",
    "    scores  : (n,)    class probability for the predicted class\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    keep : list of int indices to retain\n",
    "    \"\"\"\n",
    "    order = np.argsort(scores)[::-1]\n",
    "    keep  = []\n",
    "    while len(order) > 0:\n",
    "        i = order[0]\n",
    "        keep.append(i)\n",
    "        ious = np.array([compute_iou_1d(windows[i,0], windows[i,1],\n",
    "                                         windows[j,0], windows[j,1])\n",
    "                         for j in order[1:]])\n",
    "        order = order[1:][ious <= iou_threshold]\n",
    "    return keep\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaac31d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 7.  CONCATENATION / PREDICTION  (Algorithm 1)\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "def concatenate_segments(predicted_boundaries):\n",
    "    \"\"\"\n",
    "    Algorithm 1: merge adjacent same-class windows into activity spans.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    predicted_boundaries : list of {'activity', 'start', 'end'} sorted by start\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list of {'activity', 'start', 'end'}\n",
    "    \"\"\"\n",
    "    if not predicted_boundaries:\n",
    "        return []\n",
    "    predicted_boundaries = sorted(predicted_boundaries, key=lambda b: b['start'])\n",
    "    merged = []\n",
    "    cur = predicted_boundaries[0].copy()\n",
    "    for b in predicted_boundaries[1:]:\n",
    "        if b['activity'] == cur['activity']:\n",
    "            cur['end'] = max(cur['end'], b['end'])\n",
    "        else:\n",
    "            merged.append(cur)\n",
    "            cur = b.copy()\n",
    "    merged.append(cur)\n",
    "    return merged\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42605b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 8.  EVALUATION METRICS  (Section IV-B)\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "def levenshtein_distance(s1, s2):\n",
    "    \"\"\"Edit distance between two label sequences.\"\"\"\n",
    "    m, n = len(s1), len(s2)\n",
    "    dp = np.arange(n + 1, dtype=float)\n",
    "    for i in range(1, m + 1):\n",
    "        prev = dp.copy()\n",
    "        dp[0] = i\n",
    "        for j in range(1, n + 1):\n",
    "            dp[j] = min(prev[j] + 1,\n",
    "                        dp[j - 1] + 1,\n",
    "                        prev[j - 1] + (0 if s1[i-1] == s2[j-1] else 1))\n",
    "    return dp[n]\n",
    "\n",
    "\n",
    "def normalized_edit_distance(pred_seq, true_seq):\n",
    "    \"\"\"NED (eq. 9-10 in paper). Lower is better.\"\"\"\n",
    "    lev = levenshtein_distance(pred_seq, true_seq)\n",
    "    return lev / max(len(true_seq), 1)\n",
    "\n",
    "\n",
    "def weighted_f1(y_true, y_pred):\n",
    "    \"\"\"Weighted F1 score (eq. 11).\"\"\"\n",
    "    return f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b86b728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 9.  TRAINING\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "@tf.function\n",
    "def train_step(model, optimizer, X_batch, cls_batch, loc_batch, pos_batch):\n",
    "    with tf.GradientTape() as tape:\n",
    "        cls_pred, loc_pred = model(X_batch, training=True)\n",
    "        loss, conf_l, loc_l = mthars_loss(\n",
    "            cls_pred, loc_pred, cls_batch, loc_batch, pos_batch,\n",
    "            alpha=ALPHA, beta=BETA)\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    return loss, conf_l, loc_l\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def val_step(model, X_batch, cls_batch, loc_batch, pos_batch):\n",
    "    cls_pred, loc_pred = model(X_batch, training=False)\n",
    "    loss, conf_l, loc_l = mthars_loss(\n",
    "        cls_pred, loc_pred, cls_batch, loc_batch, pos_batch,\n",
    "        alpha=ALPHA, beta=BETA)\n",
    "    return loss, conf_l, loc_l\n",
    "\n",
    "\n",
    "def train_model(model, optimizer, train_ds, val_ds, epochs, scheduler=None):\n",
    "    history = {'train_loss': [], 'val_loss': [], 'val_conf': [], 'val_loc': []}\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # ── Training ──────────────────────────────────────────────────────────\n",
    "        t_losses = []\n",
    "        for X_b, cls_b, loc_b, pos_b in train_ds:\n",
    "            l, cl, ll = train_step(model, optimizer, X_b, cls_b, loc_b, pos_b)\n",
    "            t_losses.append(l.numpy())\n",
    "\n",
    "        # ── Validation ────────────────────────────────────────────────────────\n",
    "        v_losses, v_conf, v_loc = [], [], []\n",
    "        for X_b, cls_b, loc_b, pos_b in val_ds:\n",
    "            l, cl, ll = val_step(model, X_b, cls_b, loc_b, pos_b)\n",
    "            v_losses.append(l.numpy())\n",
    "            v_conf.append(cl.numpy())\n",
    "            v_loc.append(ll.numpy())\n",
    "\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "        history['train_loss'].append(np.mean(t_losses))\n",
    "        history['val_loss'].append(np.mean(v_losses))\n",
    "        history['val_conf'].append(np.mean(v_conf))\n",
    "        history['val_loc'].append(np.mean(v_loc))\n",
    "\n",
    "        print(f'Epoch {epoch+1:3d}/{epochs}  '\n",
    "              f'train_loss={history[\"train_loss\"][-1]:.4f}  '\n",
    "              f'val_loss={history[\"val_loss\"][-1]:.4f}  '\n",
    "              f'(conf={history[\"val_conf\"][-1]:.4f}, '\n",
    "              f'loc={history[\"val_loc\"][-1]:.4f})')\n",
    "\n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab4dcaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 10. INFERENCE PIPELINE (end-to-end segmentation + recognition)\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "def predict_stream(model, stream, feature_seq_len, scales,\n",
    "                   fixed_window_samples=600, step=300,\n",
    "                   nms_thresh=NMS_IOU_THRESHOLD,\n",
    "                   score_thresh=0.3):\n",
    "    \"\"\"\n",
    "    Run the full MTHARS inference pipeline on a continuous stream.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    all_segments : list of {'activity', 'start', 'end'}\n",
    "    \"\"\"\n",
    "    windows_template = generate_windows(feature_seq_len, scales)\n",
    "    scale_factor     = fixed_window_samples / feature_seq_len\n",
    "    scaled_windows   = windows_template.copy()\n",
    "    scaled_windows[:, 0] *= scale_factor\n",
    "    scaled_windows[:, 1] *= scale_factor\n",
    "\n",
    "    T = len(stream)\n",
    "    raw_preds = []\n",
    "\n",
    "    for start in range(0, T - fixed_window_samples + 1, step):\n",
    "        chunk  = stream[start : start + fixed_window_samples]\n",
    "        X      = chunk[np.newaxis, :, :, np.newaxis].astype(np.float32)\n",
    "        cls_p, loc_p = model(X, training=False)\n",
    "        cls_p  = cls_p.numpy()[0]    # (n_anchors, K+1)\n",
    "        loc_p  = loc_p.numpy()[0]    # (n_anchors, 2)\n",
    "\n",
    "        probs    = tf.nn.softmax(cls_p, axis=-1).numpy()   # (n_anchors, K+1)\n",
    "        pred_cls = np.argmax(probs, axis=-1)               # (n_anchors,)\n",
    "        pred_scr = probs[np.arange(len(pred_cls)), pred_cls]\n",
    "\n",
    "        # Filter background (class 0) and low-score anchors\n",
    "        fg_mask  = (pred_cls > 0) & (pred_scr >= score_thresh)\n",
    "        if not np.any(fg_mask):\n",
    "            continue\n",
    "\n",
    "        fg_wins  = scaled_windows[fg_mask]\n",
    "        fg_cls   = pred_cls[fg_mask]\n",
    "        fg_scr   = pred_scr[fg_mask]\n",
    "        fg_loc   = loc_p[fg_mask]\n",
    "\n",
    "        # Decode offsets → predicted boundaries\n",
    "        decoded  = []\n",
    "        for i in range(len(fg_wins)):\n",
    "            tx, tl = decode_offsets(fg_loc[i, 0], fg_loc[i, 1],\n",
    "                                    fg_wins[i, 0], fg_wins[i, 1])\n",
    "            decoded.append({'tx': tx, 'tl': tl,\n",
    "                            'activity': fg_cls[i], 'score': fg_scr[i]})\n",
    "\n",
    "        # Per-class NMS\n",
    "        for act_id in np.unique(fg_cls):\n",
    "            idx    = [i for i, d in enumerate(decoded) if d['activity'] == act_id]\n",
    "            w_arr  = np.array([[decoded[i]['tx'], decoded[i]['tl']] for i in idx])\n",
    "            s_arr  = np.array([decoded[i]['score'] for i in idx])\n",
    "            keep   = nms_1d(w_arr, s_arr, nms_thresh)\n",
    "            for k in keep:\n",
    "                d = decoded[idx[k]]\n",
    "                seg_start = max(0, int(d['tx'] - d['tl'] / 2) + start)\n",
    "                seg_end   = min(T - 1, int(d['tx'] + d['tl'] / 2) + start)\n",
    "                raw_preds.append({'activity': int(d['activity']),\n",
    "                                  'start': seg_start, 'end': seg_end})\n",
    "\n",
    "    # Concatenate adjacent same-class segments (Algorithm 1)\n",
    "    final_segments = concatenate_segments(raw_preds)\n",
    "    return final_segments\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a7e46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 11. EVALUATION AGAINST GROUND TRUTH\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "def evaluate(model, reconstructed_test, feature_seq_len, scales,\n",
    "             fixed_window_samples=600, step=300):\n",
    "    \"\"\"\n",
    "    Compute NED (segmentation) and weighted-F1 (recognition) on the test set.\n",
    "    \"\"\"\n",
    "    all_true_cls, all_pred_cls = [], []\n",
    "    ned_scores = []\n",
    "\n",
    "    for sub, data in reconstructed_test.items():\n",
    "        stream     = data['signal']\n",
    "        gt_bounds  = data['boundaries']\n",
    "        true_seq   = [b['activity'] for b in sorted(gt_bounds, key=lambda b: b['start'])]\n",
    "\n",
    "        pred_segs  = predict_stream(model, stream, feature_seq_len, scales,\n",
    "                                    fixed_window_samples, step)\n",
    "        pred_seq   = [s['activity'] for s in pred_segs]\n",
    "\n",
    "        ned = normalized_edit_distance(pred_seq, true_seq)\n",
    "        ned_scores.append(ned)\n",
    "\n",
    "        # Per-sample classification labels for F1\n",
    "        T = len(stream)\n",
    "        true_lbl = data['labels']                     # (T,)\n",
    "        pred_lbl = np.zeros(T, dtype=np.int32)\n",
    "        for seg in pred_segs:\n",
    "            pred_lbl[seg['start']:seg['end'] + 1] = seg['activity']\n",
    "\n",
    "        all_true_cls.extend(true_lbl.tolist())\n",
    "        all_pred_cls.extend(pred_lbl.tolist())\n",
    "\n",
    "    mean_ned = np.mean(ned_scores)\n",
    "    f1       = weighted_f1(all_true_cls, all_pred_cls)\n",
    "\n",
    "    print(f'\\n── Evaluation Results ───────────────────────────')\n",
    "    print(f'  Mean NED (↓ better) : {mean_ned:.4f}')\n",
    "    print(f'  Weighted F1 (↑)     : {f1:.4f}')\n",
    "    print(f'─────────────────────────────────────────────────')\n",
    "    print(classification_report(all_true_cls, all_pred_cls,\n",
    "                                 target_names=list(ACTIVITY_NAMES.values()),\n",
    "                                 zero_division=0))\n",
    "    return mean_ned, f1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23938652",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 12. PLOTTING\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "def plot_training_history(history, save_path=None):\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    axes[0].plot(history['train_loss'], label='Train')\n",
    "    axes[0].plot(history['val_loss'],   label='Val')\n",
    "    axes[0].set_title('Total Loss'); axes[0].legend(); axes[0].set_xlabel('Epoch')\n",
    "    axes[1].plot(history['val_conf'], label='Conf loss (val)')\n",
    "    axes[1].plot(history['val_loc'],  label='Loc loss  (val)')\n",
    "    axes[1].set_title('Val Loss Breakdown'); axes[1].legend(); axes[1].set_xlabel('Epoch')\n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=150)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def visualize_segmentation(stream, gt_bounds, pred_segs, channel=0,\n",
    "                            title='Segmentation Result', save_path=None):\n",
    "    \"\"\"Plot raw signal overlaid with ground-truth and predicted boundaries.\"\"\"\n",
    "    T = len(stream)\n",
    "    fig, ax = plt.subplots(figsize=(16, 4))\n",
    "    ax.plot(stream[:, channel], color='steelblue', lw=0.6, label='Signal')\n",
    "\n",
    "    colors = plt.cm.tab10.colors\n",
    "    for b in gt_bounds:\n",
    "        ax.axvspan(b['start'], b['end'], alpha=0.15,\n",
    "                   color=colors[(b['activity'] - 1) % 10])\n",
    "        ax.text((b['start'] + b['end']) / 2, ax.get_ylim()[1] * 0.9,\n",
    "                ACTIVITY_NAMES.get(b['activity'], str(b['activity'])),\n",
    "                ha='center', fontsize=6, color='black')\n",
    "\n",
    "    for s in pred_segs:\n",
    "        ax.axvline(s['start'], color='red',   lw=1.2, ls='--', alpha=0.7)\n",
    "        ax.axvline(s['end'],   color='orange', lw=1.2, ls='--', alpha=0.7)\n",
    "\n",
    "    ax.set_title(title); ax.set_xlabel('Sample index'); ax.legend()\n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=150)\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e075ab",
   "metadata": {},
   "outputs": [],
   "source": [
    " # ── Build datasets ────────────────────────────────────────────────────────\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 13. MAIN\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "def main():\n",
    "    # ── GPU setup ─────────────────────────────────────────────────────────────\n",
    "    for gpu in tf.config.list_physical_devices('GPU'):\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "    # ── Load raw windows ──────────────────────────────────────────────────────\n",
    "    print('==> Loading raw inertial signals …')\n",
    "    train_sig_dir = os.path.join(DATA_ROOT, 'train', 'Inertial Signals')\n",
    "    test_sig_dir  = os.path.join(DATA_ROOT, 'test',  'Inertial Signals')\n",
    "\n",
    "    X_train_w = load_raw_signals(train_sig_dir, SIGNAL_NAMES_TRAIN)  # (7352,128,9)\n",
    "    X_test_w  = load_raw_signals(test_sig_dir,  SIGNAL_NAMES_TEST)   # (2947,128,9)\n",
    "\n",
    "    y_train_w = (np.loadtxt(\n",
    "        os.path.join(DATA_ROOT, 'train', 'y_train.txt'))).astype(np.int32)\n",
    "    y_test_w  = (np.loadtxt(\n",
    "        os.path.join(DATA_ROOT, 'test',  'y_test.txt'))).astype(np.int32)\n",
    "\n",
    "    sub_train = np.loadtxt(\n",
    "        os.path.join(DATA_ROOT, 'train', 'subject_train.txt')).astype(np.int32)\n",
    "    sub_test  = np.loadtxt(\n",
    "        os.path.join(DATA_ROOT, 'test',  'subject_test.txt')).astype(np.int32)\n",
    "\n",
    "    print(f'  X_train_w: {X_train_w.shape}  y_train_w: {y_train_w.shape}')\n",
    "    print(f'  X_test_w:  {X_test_w.shape}   y_test_w:  {y_test_w.shape}')\n",
    "\n",
    "    # ── Reconstruct continuous streams ────────────────────────────────────────\n",
    "    print('==> Reconstructing continuous streams per subject …')\n",
    "    rec_train = reconstruct_continuous_stream(X_train_w, y_train_w, sub_train)\n",
    "    rec_test  = reconstruct_continuous_stream(X_test_w,  y_test_w,  sub_test)\n",
    "    print(f'  Train subjects: {len(rec_train)}  '\n",
    "          f'Test subjects: {len(rec_test)}')\n",
    "\n",
    "    # ── Derive feature-sequence length from backbone output ───────────────────\n",
    "    # A chunk of 600 samples → conv1 with stride 3 → ~200 time steps\n",
    "    FIXED_WINDOW  = 600\n",
    "    STEP          = 300\n",
    "    FEAT_SEQ_LEN  = FIXED_WINDOW // 3   # ≈ 200\n",
    "\n",
    "    n_anchors_per_pos = len(SCALES) * 2\n",
    "    n_anchors_total   = FEAT_SEQ_LEN * n_anchors_per_pos\n",
    "\n",
    "       \n",
    "    print('==> Building anchor-labelled datasets …')\n",
    "    X_tr, cls_tr, loc_tr, pos_tr = build_dataset(\n",
    "        rec_train, FEAT_SEQ_LEN, SCALES, NUM_CLASSES, FIXED_WINDOW, STEP)\n",
    "    X_te, cls_te, loc_te, pos_te = build_dataset(\n",
    "        rec_test,  FEAT_SEQ_LEN, SCALES, NUM_CLASSES, FIXED_WINDOW, STEP)\n",
    "\n",
    "    print(f'  Train chunks: {len(X_tr)}  anchors/chunk: {n_anchors_total}')\n",
    "    print(f'  Test  chunks: {len(X_te)}')\n",
    "\n",
    "    # Normalise signal across training set\n",
    "    mu  = X_tr.mean(axis=(0, 1, 3), keepdims=True)\n",
    "    std = X_tr.std(axis=(0, 1, 3), keepdims=True) + 1e-6\n",
    "    X_tr = (X_tr - mu) / std\n",
    "    X_te = (X_te - mu) / std\n",
    "\n",
    "    # tf.data pipelines\n",
    "    def make_ds(X, cls, loc, pos, shuffle=False):\n",
    "        ds = tf.data.Dataset.from_tensor_slices(\n",
    "            (X,\n",
    "             tf.cast(cls, tf.int32),\n",
    "             tf.cast(loc, tf.float32),\n",
    "             tf.cast(pos, tf.bool))\n",
    "        )\n",
    "        if shuffle:\n",
    "            ds = ds.shuffle(buffer_size=1024)\n",
    "        return ds.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    train_ds = make_ds(X_tr, cls_tr, loc_tr, pos_tr, shuffle=True)\n",
    "    val_ds   = make_ds(X_te, cls_te, loc_te, pos_te)\n",
    "\n",
    "    # ── Build model ───────────────────────────────────────────────────────────\n",
    "    print('==> Building MTHARS model …')\n",
    "    INPUT_SHAPE = (FIXED_WINDOW, 9, 1)\n",
    "    model = build_mthars(INPUT_SHAPE, NUM_CLASSES, n_anchors_per_pos,\n",
    "                          M=3, G=32, r=32, L=32)\n",
    "    model.summary()\n",
    "\n",
    "    # ── Optimizer + StepLR scheduler ─────────────────────────────────────────\n",
    "    # StepLR(step_size=50, gamma=0.1)  → implemented via LambdaCallback\n",
    "    lr_schedule = keras.optimizers.schedules.PiecewiseConstantDecay(\n",
    "        boundaries=[50 * (len(X_tr) // BATCH_SIZE),\n",
    "                    100 * (len(X_tr) // BATCH_SIZE)],\n",
    "        values=[LR, LR * 0.1, LR * 0.01]\n",
    "    )\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=lr_schedule,\n",
    "                                       weight_decay=WD)\n",
    "\n",
    "    # ── Train ─────────────────────────────────────────────────────────────────\n",
    "    print('==> Training …')\n",
    "    history = train_model(model, optimizer, train_ds, val_ds, EPOCHS)\n",
    "\n",
    "    # ── Save weights ──────────────────────────────────────────────────────────\n",
    "    model.save_weights('mthars_best.weights.h5')\n",
    "    print('Weights saved → mthars_best.weights.h5')\n",
    "\n",
    "    # ── Evaluate ─────────────────────────────────────────────────────────────\n",
    "    print('==> Evaluating on test set …')\n",
    "    mean_ned, f1 = evaluate(model, rec_test, FEAT_SEQ_LEN, SCALES,\n",
    "                             FIXED_WINDOW, STEP)\n",
    "\n",
    "    # ── Plots ─────────────────────────────────────────────────────────────────\n",
    "    plot_training_history(history, save_path='mthars_training.png')\n",
    "\n",
    "    # Visualise one test subject\n",
    "    sub_id = list(rec_test.keys())[0]\n",
    "    stream = rec_test[sub_id]['signal']\n",
    "    preds  = predict_stream(model, stream, FEAT_SEQ_LEN, SCALES, FIXED_WINDOW, STEP)\n",
    "    visualize_segmentation(stream, rec_test[sub_id]['boundaries'], preds,\n",
    "                            title=f'Subject {sub_id} – Predicted vs GT',\n",
    "                            save_path='mthars_segmentation.png')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
