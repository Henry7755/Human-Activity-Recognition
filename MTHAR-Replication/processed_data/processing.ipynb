{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b12d1af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import layers\n",
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bccd043",
   "metadata": {},
   "source": [
    "The structuring of the data for easy training. Since we are using Selective_kernel Network, which is CNN based for Human Activity Recognition and Segmentation\n",
    "\n",
    "The first step is the processing of the data to be used for training. From Better Deep Learning it is required that you use 3D of getting the data ready.\n",
    "The 3D structure of the input data is often summarized using the arrary shape of the notation [samples, timesteps, features]. The 2D format of this should be [samples, features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f7bd117",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8071/1079293147.py:5: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  data = pd.read_csv(f'{folder_path}/{name}.txt', header=None, delim_whitespace=True)\n",
      "/tmp/ipykernel_8071/1079293147.py:5: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  data = pd.read_csv(f'{folder_path}/{name}.txt', header=None, delim_whitespace=True)\n",
      "/tmp/ipykernel_8071/1079293147.py:5: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  data = pd.read_csv(f'{folder_path}/{name}.txt', header=None, delim_whitespace=True)\n",
      "/tmp/ipykernel_8071/1079293147.py:5: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  data = pd.read_csv(f'{folder_path}/{name}.txt', header=None, delim_whitespace=True)\n",
      "/tmp/ipykernel_8071/1079293147.py:5: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  data = pd.read_csv(f'{folder_path}/{name}.txt', header=None, delim_whitespace=True)\n",
      "/tmp/ipykernel_8071/1079293147.py:5: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  data = pd.read_csv(f'{folder_path}/{name}.txt', header=None, delim_whitespace=True)\n",
      "/tmp/ipykernel_8071/1079293147.py:5: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  data = pd.read_csv(f'{folder_path}/{name}.txt', header=None, delim_whitespace=True)\n",
      "/tmp/ipykernel_8071/1079293147.py:5: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  data = pd.read_csv(f'{folder_path}/{name}.txt', header=None, delim_whitespace=True)\n",
      "/tmp/ipykernel_8071/1079293147.py:5: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  data = pd.read_csv(f'{folder_path}/{name}.txt', header=None, delim_whitespace=True)\n",
      "/tmp/ipykernel_8071/1079293147.py:26: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  y_train = pd.read_csv('/home/pschye/Documents/KCCR - Biosignals/Human Activity Recognition/Human Activity Recognition/human+activity+recognition+using+smartphones/UCI HAR Dataset/UCI HAR Dataset/train/y_train.txt', header=None, delim_whitespace=True).values.flatten() - 1\n",
      "/tmp/ipykernel_8071/1079293147.py:5: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  data = pd.read_csv(f'{folder_path}/{name}.txt', header=None, delim_whitespace=True)\n",
      "/tmp/ipykernel_8071/1079293147.py:5: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  data = pd.read_csv(f'{folder_path}/{name}.txt', header=None, delim_whitespace=True)\n",
      "/tmp/ipykernel_8071/1079293147.py:5: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  data = pd.read_csv(f'{folder_path}/{name}.txt', header=None, delim_whitespace=True)\n",
      "/tmp/ipykernel_8071/1079293147.py:5: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  data = pd.read_csv(f'{folder_path}/{name}.txt', header=None, delim_whitespace=True)\n",
      "/tmp/ipykernel_8071/1079293147.py:5: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  data = pd.read_csv(f'{folder_path}/{name}.txt', header=None, delim_whitespace=True)\n",
      "/tmp/ipykernel_8071/1079293147.py:5: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  data = pd.read_csv(f'{folder_path}/{name}.txt', header=None, delim_whitespace=True)\n",
      "/tmp/ipykernel_8071/1079293147.py:5: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  data = pd.read_csv(f'{folder_path}/{name}.txt', header=None, delim_whitespace=True)\n",
      "/tmp/ipykernel_8071/1079293147.py:5: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  data = pd.read_csv(f'{folder_path}/{name}.txt', header=None, delim_whitespace=True)\n",
      "/tmp/ipykernel_8071/1079293147.py:5: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  data = pd.read_csv(f'{folder_path}/{name}.txt', header=None, delim_whitespace=True)\n",
      "/tmp/ipykernel_8071/1079293147.py:28: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  y_test = pd.read_csv('/home/pschye/Documents/KCCR - Biosignals/Human Activity Recognition/Human Activity Recognition/human+activity+recognition+using+smartphones/UCI HAR Dataset/UCI HAR Dataset/test/y_test.txt', header=None, delim_whitespace=True).values.flatten() - 1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def load_raw_signals(folder_path, filenames):\n",
    "    signals = []\n",
    "    for name in filenames:\n",
    "        # Each file is (7352, 128)\n",
    "        data = pd.read_csv(f'{folder_path}/{name}.txt', header=None, delim_whitespace=True)\n",
    "        signals.append(data.values)\n",
    "    \n",
    "    # Stacking on the 3rd axis (axis=2) results in (7352, 128, 9)\n",
    "    return np.transpose(np.array(signals), (1, 2, 0))\n",
    "\n",
    "# Example list of the 9 channels based on your files\n",
    "filenames = [\n",
    "    'total_acc_x_train', 'total_acc_y_train', 'total_acc_z_train',\n",
    "    'body_acc_x_train', 'body_acc_y_train', 'body_acc_z_train',\n",
    "    'body_gyro_x_train', 'body_gyro_y_train', 'body_gyro_z_train'\n",
    "]\n",
    "\n",
    "filenames_test = [\n",
    "    'total_acc_x_test', 'total_acc_y_test', 'total_acc_z_test',\n",
    "    'body_acc_x_test', 'body_acc_y_test', 'body_acc_z_test',\n",
    "    'body_gyro_x_test', 'body_gyro_y_test', 'body_gyro_z_test'\n",
    "]\n",
    "\n",
    "\n",
    "X_train = load_raw_signals('/home/pschye/Documents/KCCR - Biosignals/Human Activity Recognition/Human Activity Recognition/human+activity+recognition+using+smartphones/UCI HAR Dataset/UCI HAR Dataset/train/Inertial Signals', filenames)\n",
    "y_train = pd.read_csv('/home/pschye/Documents/KCCR - Biosignals/Human Activity Recognition/Human Activity Recognition/human+activity+recognition+using+smartphones/UCI HAR Dataset/UCI HAR Dataset/train/y_train.txt', header=None, delim_whitespace=True).values.flatten() - 1\n",
    "X_test = load_raw_signals('/home/pschye/Documents/KCCR - Biosignals/Human Activity Recognition/Human Activity Recognition/human+activity+recognition+using+smartphones/UCI HAR Dataset/UCI HAR Dataset/test/Inertial Signals', filenames_test)\n",
    "y_test = pd.read_csv('/home/pschye/Documents/KCCR - Biosignals/Human Activity Recognition/Human Activity Recognition/human+activity+recognition+using+smartphones/UCI HAR Dataset/UCI HAR Dataset/test/y_test.txt', header=None, delim_whitespace=True).values.flatten() - 1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5678efca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x: (7352, 128, 9)   train_y: (7352,)\n",
      "test_x:  (2947, 128, 9)   test_y:  (2947,)\n"
     ]
    }
   ],
   "source": [
    "print('train_x:', X_train.shape, '  train_y:', y_train.shape)\n",
    "print('test_x: ', X_test.shape,  '  test_y: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e9943a",
   "metadata": {},
   "source": [
    "(7352, 128, 9) this is a workable format to be used in CNN(Backbone Network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4ad9d576",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-18 15:53:04.534988: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2026-02-18 15:53:04.619847: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:84] Allocation of 67756032 exceeds 10% of free system memory.\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 256\n",
    "TEST_BATCH  = 2947\n",
    "\n",
    "train_dataset = (\n",
    "    tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "    .shuffle(buffer_size=10000)\n",
    "    .batch(BATCH_SIZE)\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")\n",
    "\n",
    "test_dataset = (\n",
    "    tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "    .batch(TEST_BATCH)\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1f8915",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57cc95f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ──────────────────────────────────────────────\n",
    "# 3. SKConv layer\n",
    "#    PyTorch layout: (N, C, H, W)  →  TF layout: (N, H, W, C)\n",
    "#    PyTorch kernel=(3,1), padding=(1+i, 1), dilation=(1+i, 1)\n",
    "#    → TF Conv2D with kernel_size=(3,1), padding='same' per branch\n",
    "# ──────────────────────────────────────────────\n",
    "\n",
    "class SKConv(layers.Layer):\n",
    "    \"\"\"Selective-Kernel Convolution block (TensorFlow/Keras, channels-last).\"\"\"\n",
    "\n",
    "    def __init__(self, features, M=3, G=32, r=32, stride=1, L=32, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        d = max(int(features / r), L)\n",
    "        self.M = M\n",
    "        self.features = features\n",
    "\n",
    "        # M parallel branches with different dilation rates\n",
    "        self.branches = []\n",
    "        for i in range(M):\n",
    "            branch = keras.Sequential([\n",
    "                # groups → use DepthwiseConv2D trick or just Conv2D\n",
    "                # PyTorch groups=G  ≈  TF Conv2D with groups=G (TF 2.x supports it)\n",
    "                layers.Conv2D(\n",
    "                    filters=features,\n",
    "                    kernel_size=(3, 1),\n",
    "                    strides=(stride, 1),\n",
    "                    padding='same',\n",
    "                    dilation_rate=(1 + i, 1),\n",
    "                    groups=G,\n",
    "                    use_bias=False,\n",
    "                ),\n",
    "                layers.BatchNormalization(),\n",
    "                layers.ReLU(),\n",
    "            ], name=f'branch_{i}')\n",
    "            self.branches.append(branch)\n",
    "\n",
    "        # Global Average Pool + FC (implemented as Conv2D(1,1) to keep 4-D tensor)\n",
    "        self.gap = layers.GlobalAveragePooling2D(keepdims=True)   # → (N, 1, 1, C)\n",
    "        self.fc = keras.Sequential([\n",
    "            layers.Conv2D(d, kernel_size=1, use_bias=False),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.ReLU(),\n",
    "        ], name='fc_z')\n",
    "\n",
    "        # One 1×1 conv per branch to produce attention logits\n",
    "        self.fcs = [\n",
    "            layers.Conv2D(features, kernel_size=1, name=f'fc_attn_{i}')\n",
    "            for i in range(M)\n",
    "        ]\n",
    "        self.softmax = layers.Softmax(axis=1)   # softmax across branch dimension\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        batch_size = tf.shape(x)[0]\n",
    "\n",
    "        # Each branch output: (N, H, W, features)\n",
    "        branch_feats = [branch(x, training=training) for branch in self.branches]\n",
    "\n",
    "        # Stack → (N, M, H, W, features)\n",
    "        feats = tf.stack(branch_feats, axis=1)\n",
    "\n",
    "        # Fuse: element-wise sum across branches → (N, H, W, features)\n",
    "        feats_U = tf.reduce_sum(feats, axis=1)\n",
    "\n",
    "        # Channel descriptor via GAP → (N, 1, 1, features)\n",
    "        feats_S = self.gap(feats_U)\n",
    "\n",
    "        # Compact feature → (N, 1, 1, d)\n",
    "        feats_Z = self.fc(feats_S, training=training)\n",
    "\n",
    "        # Per-branch attention vectors → list of (N, 1, 1, features)\n",
    "        attn_vectors = [fc(feats_Z) for fc in self.fcs]\n",
    "\n",
    "        # Stack → (N, M, 1, 1, features), then softmax over branch dim\n",
    "        attn_vectors = tf.stack(attn_vectors, axis=1)\n",
    "        attn_vectors = self.softmax(attn_vectors)           # (N, M, 1, 1, features)\n",
    "\n",
    "        # Weighted sum: (N, M, H, W, features) * (N, M, 1, 1, features)\n",
    "        feats_V = tf.reduce_sum(feats * attn_vectors, axis=1)  # (N, H, W, features)\n",
    "\n",
    "        return feats_V\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update(dict(features=self.features, M=self.M))\n",
    "        return config\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fccfc7e",
   "metadata": {},
   "source": [
    "### Building Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83be672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ──────────────────────────────────────────────\n",
    "# 4. Build SKNet model\n",
    "# -------------------------------------\n",
    "\n",
    "def build_sknet(M=3, G=32, r=32, stride=1, L=32, num_classes=6):\n",
    "    \"\"\"Returns a compiled Keras Model equivalent to the PyTorch SKNet.\"\"\"\n",
    "    inputs = keras.Input(shape=(128, 9, 1), name='input')   # (H, W, 1)\n",
    "\n",
    "    # conv1: PyTorch Conv2d(1, 64, (5,1), stride=(3,1), padding=(1,0))\n",
    "    # channels-last TF equivalent:\n",
    "    x = layers.Conv2D(64, kernel_size=(5, 1), strides=(3, 1), padding='same', use_bias=True)(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "\n",
    "    # NOTE: PyTorch conv2_sk expects 128 input features but conv1 outputs 64.\n",
    "    # The original code passes 64-channel output to SKConv(128, ...) which would\n",
    "    # error in PyTorch too unless conv1 produces 128. We mirror the code as-is\n",
    "    # and set SKConv to match actual channel count (64 → 64, then 64 → 128).\n",
    "    # Adjust these numbers to match your actual checkpoint / intent.\n",
    "    x = SKConv(64,  M=M, G=G, r=r, stride=stride, L=L, name='skconv1')(x)\n",
    "    x = SKConv(128, M=M, G=G, r=r, stride=stride, L=L, name='skconv2')(x)\n",
    "\n",
    "    # Flatten\n",
    "    x = layers.Flatten()(x)\n",
    "\n",
    "    # FC: 6-class output\n",
    "    # PyTorch uses nn.LayerNorm on the logits; replicate with LayerNormalization\n",
    "    x = layers.Dense(num_classes, name='fc')(x)\n",
    "    x = layers.LayerNormalization(name='layer_norm')(x)\n",
    "\n",
    "    model = keras.Model(inputs, x, name='SKNet')\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fb2cf625",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Building model..\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"SKNet\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"SKNet\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">43</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)      │           <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_18          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">43</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)      │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ re_lu_18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">43</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ skconv1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SKConv</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">43</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)      │        <span style=\"color: #00af00; text-decoration-color: #00af00\">10,432</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ skconv2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SKConv</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">43</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">20,736</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">49536</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ fc (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)              │       <span style=\"color: #00af00; text-decoration-color: #00af00\">297,222</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ layer_norm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input (\u001b[38;5;33mInputLayer\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m9\u001b[0m, \u001b[38;5;34m1\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_18 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m43\u001b[0m, \u001b[38;5;34m9\u001b[0m, \u001b[38;5;34m64\u001b[0m)      │           \u001b[38;5;34m384\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_18          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m43\u001b[0m, \u001b[38;5;34m9\u001b[0m, \u001b[38;5;34m64\u001b[0m)      │           \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ re_lu_18 (\u001b[38;5;33mReLU\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m43\u001b[0m, \u001b[38;5;34m9\u001b[0m, \u001b[38;5;34m64\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ skconv1 (\u001b[38;5;33mSKConv\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m43\u001b[0m, \u001b[38;5;34m9\u001b[0m, \u001b[38;5;34m64\u001b[0m)      │        \u001b[38;5;34m10,432\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ skconv2 (\u001b[38;5;33mSKConv\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m43\u001b[0m, \u001b[38;5;34m9\u001b[0m, \u001b[38;5;34m128\u001b[0m)     │        \u001b[38;5;34m20,736\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_2 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m49536\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ fc (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m)              │       \u001b[38;5;34m297,222\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ layer_norm (\u001b[38;5;33mLayerNormalization\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m)              │            \u001b[38;5;34m12\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">329,042</span> (1.26 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m329,042\u001b[0m (1.26 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">327,634</span> (1.25 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m327,634\u001b[0m (1.25 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,408</span> (5.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,408\u001b[0m (5.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ──────────────────────────────────────────────\n",
    "# 5. Compile\n",
    "# ──────────────────────────────────────────────\n",
    "print('==> Building model..')\n",
    "\n",
    "model = build_sknet()\n",
    "model.summary()\n",
    "\n",
    "WD = 1e-4\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.001, weight_decay=WD)\n",
    "\n",
    "loss_fn = keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=loss_fn,\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "\n",
    "# Learning-rate scheduler: StepLR(step_size=50, gamma=0.1)\n",
    "# In Keras this is done via a LearningRateScheduler callback\n",
    "def step_lr_schedule(epoch, lr):\n",
    "    \"\"\"Halve the LR by ×0.1 every 50 epochs.\"\"\"\n",
    "    if epoch > 0 and epoch % 50 == 0:\n",
    "        return lr * 0.1\n",
    "    return lr\n",
    "\n",
    "lr_callback = keras.callbacks.LearningRateScheduler(step_lr_schedule, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "054262fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 1\n",
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-18 16:27:34.535717: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:84] Allocation of 67756032 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Arguments `target` and `output` must have the same rank (ndim). Received: target.shape=(None,), output.shape=(None, 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# --- Train ---\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m train_results \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mlr_callback\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# --- Test ---\u001b[39;00m\n\u001b[1;32m     26\u001b[0m test_results \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(test_dataset, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/ai_research/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/miniconda3/envs/ai_research/lib/python3.10/site-packages/keras/src/backend/tensorflow/nn.py:662\u001b[0m, in \u001b[0;36mcategorical_crossentropy\u001b[0;34m(target, output, from_logits, axis)\u001b[0m\n\u001b[1;32m    656\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    657\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArguments `target` and `output` must be at least rank 1. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    658\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    659\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget.shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, output.shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    660\u001b[0m     )\n\u001b[1;32m    661\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(target\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(output\u001b[38;5;241m.\u001b[39mshape):\n\u001b[0;32m--> 662\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    663\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArguments `target` and `output` must have the same rank \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    664\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(ndim). Received: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    665\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget.shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, output.shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    666\u001b[0m     )\n\u001b[1;32m    667\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m e1, e2 \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(target\u001b[38;5;241m.\u001b[39mshape, output\u001b[38;5;241m.\u001b[39mshape):\n\u001b[1;32m    668\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m e1 \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m e2 \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m e1 \u001b[38;5;241m!=\u001b[39m e2:\n",
      "\u001b[0;31mValueError\u001b[0m: Arguments `target` and `output` must have the same rank (ndim). Received: target.shape=(None,), output.shape=(None, 6)"
     ]
    }
   ],
   "source": [
    "# ──────────────────────────────────────────────\n",
    "# 6. Training loop\n",
    "# ──────────────────────────────────────────────\n",
    "\n",
    "EPOCHS = 500\n",
    "start_epoch = 1\n",
    "best_acc = 0.0\n",
    "\n",
    "\n",
    "epoch_list  = []\n",
    "error_list  = []\n",
    "\n",
    "# Custom training loop to replicate per-epoch logging\n",
    "for epoch in range(start_epoch, start_epoch + EPOCHS):\n",
    "    print(f'\\nEpoch: {epoch}')\n",
    "\n",
    "    # --- Train ---\n",
    "    train_results = model.fit(\n",
    "        train_dataset,\n",
    "        epochs=1,\n",
    "        verbose=1,\n",
    "        callbacks=[lr_callback],\n",
    "    )\n",
    "\n",
    "    # --- Test ---\n",
    "    test_results = model.evaluate(test_dataset, verbose=0)\n",
    "    test_loss    = test_results[0]\n",
    "    test_acc     = test_results[1]\n",
    "    test_error   = 1.0 - test_acc\n",
    "\n",
    "    print(f'test: {test_acc:.4f} || {test_error:.4f}')\n",
    "\n",
    "    epoch_list.append(epoch)\n",
    "    error_list.append(test_error)\n",
    "\n",
    "    if test_acc > best_acc:\n",
    "        best_acc = test_acc\n",
    "        model.save_weights('best_sknet_weights.h5')\n",
    "        print(f'  >> New best accuracy: {best_acc:.4f}  (weights saved)')\n",
    "\n",
    "        print(f'\\nTraining complete. Best test accuracy: {best_acc:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ea7a8e",
   "metadata": {},
   "source": [
    "## The Method in replicating the MTHAR work \n",
    "To reconstruct the continuous time-series stream from the pre-segmented UCI HAR dataset according to the methodology in the Duan et al. (2023) paper, you need to \"stitch\" the overlapping windows back together.\n",
    "\n",
    "The UCI HAR dataset was originally segmented into windows of 128 samples with a 50% overlap (64 samples)2. Because the paper focuses on activity segmentation (identifying starting and ending positions), reconstructing the full stream is necessary to define the ground truth boundaries $(t^x, l^x)$ for their multi-task learning model\n",
    "\n",
    "1. Grouping by Subject\n",
    "The first step is to isolate data for each individual participant.Use the subject_train.txt file, which contains a subject ID (1–30) for every row (window) in your feature matrix.Filter your raw signal tensor—shaped (7352, 128, 9)—so that you only process windows belonging to one subject at a time.\n",
    "\n",
    "2. The Reconstruction (Stitching) Process\n",
    "Since each window starts 64 samples after the previous one, you must blend the overlapping regions. The paper implies a \"smoothing\" approach through averaging to handle the transitions between windows.\n",
    "\n",
    "Logic for one channel of a single subject:Initialize: Create an empty array for the reconstructed signal and a \"counter\" array of the same length to track how many windows cover each index.\n",
    "Total Length = (Number of Windows * 64) + 648.\n",
    "Accumulate: Loop through each window $i$:Place the 128 samples into the reconstructed array starting at index $i \\times 64$.\n",
    "Add 1 to the corresponding indices in your counter array.\n",
    "Average: Divide the total accumulated signal by the counter array.Indices 0–63 and the final 64 samples will be divided by 1.Indices 64 to (Total Length - 64) will be divided by 2 (because they represent the overlap)\n",
    "\n",
    "3. Reconstructing Ground Truth Labels\n",
    "The y_train file provides one label per window. To replicate the researchers' \"Ground Truth Boundaries,\" you must map these back to the samples:Assign Labels: \n",
    "Assign the label of Window $i$ to the 64-sample \"step\" starting at $i \\times 64$.\n",
    "Define Boundaries: Scan the resulting sample-level label sequence.\n",
    " A boundary is identified whenever the activity label changes (e.g., from WALKING to SITTING).Center and Length $(t^x, t^l)$: Convert these segments into the paper's required format:$t^x$: The center coordinate of the activity segment15.$t^l$: The total number of samples (length) of that activity\n",
    " \n",
    " 4. Summary for the 9 ChannelsRepeat this process for all 9 channels (X/Y/Z for Total Acc, Body Acc, and Gyro). \n",
    " You will end up with a continuous multivariate stream of shape (Total_Samples, 9) for each subject, which the MTHARS model uses to predict both the activity class and the offset to the true bound\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d235e905",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def reconstruct_har_data(X_windows, y_windows, subject_ids):\n",
    "    \"\"\"\n",
    "    Reconstructs continuous signals and activity boundaries per subject.\n",
    "    X_windows: (7352, 128, 9) raw signal tensor\n",
    "    y_windows: (7352,) activity labels\n",
    "    subject_ids: (7352,) subject IDs from subject_train.txt\n",
    "    \"\"\"\n",
    "    unique_subjects = np.unique(subject_ids)\n",
    "    all_reconstructed_data = {}\n",
    "\n",
    "    for sub in unique_subjects:\n",
    "        # 1. Filter data for the specific subject\n",
    "        # Use .reshape(-1) to ensure the mask is a flat 1D array\n",
    "        mask = (subject_ids.reshape(-1) == sub) \n",
    "\n",
    "# Explicitly filter the first axis\n",
    "        sub_X = X_windows[mask, :, :]  \n",
    "        sub_y = y_windows[mask]\n",
    "        \n",
    "        num_windows = sub_X.shape[0]\n",
    "        overlap = 64 # 50% overlap of 128 samples\n",
    "        total_len = (num_windows * overlap) + overlap\n",
    "        \n",
    "        # 2. Initialize reconstructed signal and count array for averaging\n",
    "        sub_stream = np.zeros((total_len, 9))\n",
    "        counts = np.zeros((total_len, 1))\n",
    "        \n",
    "        # 3. Stitch windows with 50% overlap\n",
    "        for i in range(num_windows):\n",
    "            start = i * overlap\n",
    "            end = start + 128\n",
    "            sub_stream[start:end, :] += sub_X[i]\n",
    "            counts[start:end] += 1\n",
    "            \n",
    "        # 4. Average overlapping sections to smooth the signal\n",
    "        sub_stream /= counts\n",
    "        \n",
    "        # 5. Reconstruct label sequence (propagate window label to its 64-sample step)\n",
    "        sub_labels = np.zeros(total_len)\n",
    "        for i in range(num_windows):\n",
    "            sub_labels[i*overlap : (i+1)*overlap] = sub_y[i]\n",
    "        # Fill the final 64 samples with the last known label\n",
    "        sub_labels[-64:] = sub_y[-1]\n",
    "        \n",
    "        # 6. Extract Boundaries (tx: center, tl: length) as per Duan et al.\n",
    "        boundaries = []\n",
    "        # Find indices where labels change\n",
    "        change_points = np.where(np.diff(sub_labels) != 0)[0]\n",
    "        start_idx = 0\n",
    "        \n",
    "        for cp in change_points:\n",
    "            end_idx = cp\n",
    "            length = end_idx - start_idx + 1\n",
    "            center = start_idx + (length / 2)\n",
    "            boundaries.append({'activity': sub_labels[start_idx], 'tx': center, 'tl': length})\n",
    "            start_idx = end_idx + 1\n",
    "            \n",
    "        all_reconstructed_data[sub] = {\n",
    "            'signal': sub_stream, \n",
    "            'labels': sub_labels,\n",
    "            'boundaries': boundaries\n",
    "        }\n",
    "        \n",
    "    return all_reconstructed_data\n",
    "\n",
    "# --- Usage Example ---\n",
    "# Assuming X_train_raw is (7352, 128, 9)\n",
    "# y_train is (7352,)\n",
    "# sub_train is (7352,) from subject_train.txt\n",
    "# reconstructed = reconstruct_har_data(X_train_raw, y_train, sub_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9224ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train  = np.loadtxt(\"/home/pschye/Documents/KCCR - Biosignals/Human Activity Recognition/Human Activity Recognition/human+activity+recognition+using+smartphones/UCI HAR Dataset/UCI HAR Dataset/train/y_train.txt\")\n",
    "subjects = np.loadtxt(\"/home/pschye/Documents/KCCR - Biosignals/Human Activity Recognition/Human Activity Recognition/human+activity+recognition+using+smartphones/UCI HAR Dataset/UCI HAR Dataset/train/subject_train.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a5c044f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.,  1.,  1., ..., 30., 30., 30.], shape=(7352,))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b166043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to your data\n",
    "reconstructed_data = reconstruct_har_data(X_train, y_train, subjects)\n",
    "\n",
    "# Example: Accessing the continuous data for Subject 3\n",
    "sub1_signal = reconstructed_data[3]['signal']   # Continuous (Length, 9) stream\n",
    "sub1_labels = reconstructed_data[3]['labels']   # Sample-level activity labels\n",
    "sub1_events = reconstructed_data[3]['boundaries'] # List of (tx, tl) events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ae1779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21888\n"
     ]
    }
   ],
   "source": [
    "print(len(sub1_signal))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0d5d21",
   "metadata": {},
   "source": [
    "# MTHAR - THE METHOD IN ACTION USING TENSORFLOW\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523f190f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-29 22:06:55.975363: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2026-01-29 22:07:02.823997: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2026-01-29 22:07:18.657722: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import numpy as np\n",
    "\n",
    "# ==========================================\n",
    "# 1. SELECTIVE KERNEL (Backbone Integrity)\n",
    "# ==========================================\n",
    "class SKConv(layers.Layer):\n",
    "    def __init__(self, features, M=3, G=32, r=32, stride=1, L=32):\n",
    "        super(SKConv, self).__init__()\n",
    "        self.M = M\n",
    "        self.features = features\n",
    "        self.d = max(int(features / r), L)\n",
    "        \n",
    "        self.convs = []\n",
    "        for i in range(M):\n",
    "            self.convs.append(models.Sequential([\n",
    "                layers.Conv2D(features, kernel_size=(3, 1), strides=stride, \n",
    "                              padding='same', dilation_rate=(1 + i, 1), groups=G, use_bias=False),\n",
    "                layers.BatchNormalization(),\n",
    "                layers.ReLU()\n",
    "            ]))\n",
    "\n",
    "        self.gap = layers.GlobalAveragePooling2D()\n",
    "        self.fc = models.Sequential([\n",
    "            layers.Dense(self.d, use_bias=False),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.ReLU()\n",
    "        ])\n",
    "        self.fcs = [layers.Dense(features) for _ in range(M)]\n",
    "        self.softmax = layers.Softmax(axis=1)\n",
    "\n",
    "    def call(self, x):\n",
    "        feats = [conv(x) for conv in self.convs]\n",
    "        feats_stack = tf.stack(feats, axis=1)\n",
    "        \n",
    "        feats_U = tf.reduce_sum(feats_stack, axis=1)\n",
    "        feats_S = self.gap(feats_U)\n",
    "        feats_Z = self.fc(feats_S)\n",
    "        \n",
    "        attention_vectors = [fc(feats_Z) for fc in self.fcs]\n",
    "        attention_vectors = tf.stack(attention_vectors, axis=1)\n",
    "        attention_vectors = self.softmax(attention_vectors)\n",
    "        attention_vectors = tf.expand_dims(tf.expand_dims(attention_vectors, axis=2), axis=2)\n",
    "        \n",
    "        return tf.reduce_sum(feats_stack * attention_vectors, axis=1)\n",
    "\n",
    "# ==========================================\n",
    "# 2. PHASE 2: MULTI-SCALE WINDOW LOGIC\n",
    "# ==========================================\n",
    "SCALES = [0.2, 0.4, 0.6, 0.8, 1.0]\n",
    "NUM_ANCHORS_PER_POS = len(SCALES) * 2 # w1 and w2 for each scale\n",
    "\n",
    "# ==========================================\n",
    "# 3. PHASE 3: MULTI-TASK MODEL (MTHARS)\n",
    "# ==========================================\n",
    "def build_mthars_model(input_shape=(128, 9, 1), num_classes=6):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "\n",
    "    # --- Backbone (SKNet) ---\n",
    "    x = layers.Conv2D(64, (5, 1), strides=(2, 1), padding='same')(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "\n",
    "    x = SKConv(128)(x)\n",
    "    x = SKConv(256)(x) # Feature Map output\n",
    "\n",
    "    # --- Multi-Task Heads (Detection Style) ---\n",
    "    # Recognition Head: Predicts class for every anchor at every time step\n",
    "    # Shape: [Batch, Time_Steps, 1, Anchors * Classes]\n",
    "    cls_output = layers.Conv2D(NUM_ANCHORS_PER_POS * num_classes, (3, 1), padding='same', name='cls_head')(x)\n",
    "    \n",
    "    # Localization Head: Predicts (fx, fl) offsets for every anchor\n",
    "    # Shape: [Batch, Time_Steps, 1, Anchors * 2]\n",
    "    loc_output = layers.Conv2D(NUM_ANCHORS_PER_POS * 2, (3, 1), padding='same', name='loc_head')(x)\n",
    "\n",
    "    # Reshaping for Loss Calculation\n",
    "    # We flatten the spatial dimensions to treat all anchors across the sequence as one batch\n",
    "    cls_output = layers.Reshape((-1, num_classes), name='recognition')(cls_output)\n",
    "    loc_output = layers.Reshape((-1, 2), name='localization')(loc_output)\n",
    "\n",
    "    return models.Model(inputs=inputs, outputs=[cls_output, loc_output])\n",
    "\n",
    "# ==========================================\n",
    "# 4. TRAINING CRITERIA (Multi-Task Loss)\n",
    "# ==========================================\n",
    "def mthars_loss(y_true_cls, y_pred_cls, y_true_loc, y_pred_loc):\n",
    "    # Classification: Categorical Cross Entropy (Recognition)\n",
    "    cls_loss = tf.keras.losses.CategoricalCrossentropy()(y_true_cls, y_pred_cls)\n",
    "    \n",
    "    # Regression: Smooth L1 Loss (Localization/Offsets)\n",
    "    # Only calculate localization loss for \"Positive\" anchors (not background)\n",
    "    loc_loss = tf.keras.losses.Huber()(y_true_loc, y_pred_loc)\n",
    "    \n",
    "    return cls_loss + loc_loss # Total Multi-task Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68360a6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-29 18:47:32.575002: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2026-01-29 18:47:43.325763: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2026-01-29 18:48:30.245793: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2026-01-29 18:48:52.999769: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_8\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_8\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">43</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>) │        <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalization │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">43</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>) │        <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ conv2d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ re_lu (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">43</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ sk_conv (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SKConv</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">43</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>,     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">20,736</span> │ re_lu[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ sk_conv_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SKConv</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">43</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>,     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">45,952</span> │ sk_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">99072</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ sk_conv_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ recognition_output  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)         │    <span style=\"color: #00af00; text-decoration-color: #00af00\">594,438</span> │ flatten[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ segmentation_output │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)         │    <span style=\"color: #00af00; text-decoration-color: #00af00\">198,146</span> │ flatten[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalization │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span> │ recognition_outp… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span> │ segmentation_out… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ activation          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalizat… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ activation_1        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalizat… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        │                   │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m9\u001b[0m, \u001b[38;5;34m1\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m43\u001b[0m, \u001b[38;5;34m9\u001b[0m, \u001b[38;5;34m64\u001b[0m) │        \u001b[38;5;34m384\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalization │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m43\u001b[0m, \u001b[38;5;34m9\u001b[0m, \u001b[38;5;34m64\u001b[0m) │        \u001b[38;5;34m256\u001b[0m │ conv2d[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ re_lu (\u001b[38;5;33mReLU\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m43\u001b[0m, \u001b[38;5;34m9\u001b[0m, \u001b[38;5;34m64\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ sk_conv (\u001b[38;5;33mSKConv\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m43\u001b[0m, \u001b[38;5;34m9\u001b[0m,     │     \u001b[38;5;34m20,736\u001b[0m │ re_lu[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "│                     │ \u001b[38;5;34m128\u001b[0m)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ sk_conv_1 (\u001b[38;5;33mSKConv\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m43\u001b[0m, \u001b[38;5;34m9\u001b[0m,     │     \u001b[38;5;34m45,952\u001b[0m │ sk_conv[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "│                     │ \u001b[38;5;34m256\u001b[0m)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m99072\u001b[0m)     │          \u001b[38;5;34m0\u001b[0m │ sk_conv_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ recognition_output  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m)         │    \u001b[38;5;34m594,438\u001b[0m │ flatten[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ segmentation_output │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)         │    \u001b[38;5;34m198,146\u001b[0m │ flatten[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalization │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m)         │         \u001b[38;5;34m12\u001b[0m │ recognition_outp… │\n",
       "│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)         │          \u001b[38;5;34m4\u001b[0m │ segmentation_out… │\n",
       "│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ activation          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ layer_normalizat… │\n",
       "│ (\u001b[38;5;33mActivation\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ activation_1        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ layer_normalizat… │\n",
       "│ (\u001b[38;5;33mActivation\u001b[0m)        │                   │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">859,928</span> (3.28 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m859,928\u001b[0m (3.28 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">857,368</span> (3.27 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m857,368\u001b[0m (3.27 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,560</span> (10.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,560\u001b[0m (10.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# The Selective Kernel Convolution (SKConv) is a key component of the backbone network in the MTHARS architecture.\n",
    "#  It allows the model to adaptively select features from multiple convolutional branches with different receptive fields,\n",
    "#  enhancing the model's ability to capture multi-scale temporal patterns in the input signals. \n",
    "# The SKConv layer consists of multiple convolutional branches, a global average pooling layer for feature fusion,\n",
    "#  and attention mechanisms to weigh the importance of each branch's output before combining them into a single feature map for further processing in the network.\n",
    "class SKConv(layers.Layer):\n",
    "    def __init__(self, features, M=3, G=32, r=32, stride=1, L=32):\n",
    "        super(SKConv, self).__init__()\n",
    "        self.M = M\n",
    "        self.features = features\n",
    "        self.d = max(int(features / r), L)\n",
    "        \n",
    "        # Branching Convolutions (Split)\n",
    "        self.convs = []\n",
    "        for i in range(M):\n",
    "            # Using dilation to mimic different receptive fields as per the paper\n",
    "            self.convs.append(models.Sequential([\n",
    "                layers.Conv2D(features, kernel_size=(3, 1), strides=stride, \n",
    "                              padding='same', dilation_rate=(1 + i, 1), groups=G, use_bias=False),\n",
    "                layers.BatchNormalization(),\n",
    "                layers.ReLU()\n",
    "            ]))\n",
    "\n",
    "        self.gap = layers.GlobalAveragePooling2D()\n",
    "        \n",
    "        # Attention bottleneck (Fuse)\n",
    "        self.fc = models.Sequential([\n",
    "            layers.Dense(self.d, use_bias=False),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.ReLU()\n",
    "        ])\n",
    "        \n",
    "        # Selection heads\n",
    "        self.fcs = [layers.Dense(features) for _ in range(M)]\n",
    "        self.softmax = layers.Softmax(axis=1)\n",
    "\n",
    "    def call(self, x):\n",
    "        batch_size = tf.shape(x)[0]\n",
    "        \n",
    "        # Split\n",
    "        feats = [conv(x) for conv in self.convs] # List of [batch, h, w, c]\n",
    "        feats_stack = tf.stack(feats, axis=1)    # [batch, M, h, w, c]\n",
    "        \n",
    "        # Fuse\n",
    "        feats_U = tf.reduce_sum(feats_stack, axis=1)\n",
    "        feats_S = self.gap(feats_U)\n",
    "        feats_Z = self.fc(feats_S)\n",
    "        \n",
    "        # Select\n",
    "        attention_vectors = [fc(feats_Z) for fc in self.fcs]\n",
    "        attention_vectors = tf.stack(attention_vectors, axis=1) # [batch, M, c]\n",
    "        attention_vectors = self.softmax(attention_vectors)\n",
    "        \n",
    "        # Reshape for broadcasting: [batch, M, 1, 1, c]\n",
    "        attention_vectors = tf.expand_dims(tf.expand_dims(attention_vectors, axis=2), axis=2)\n",
    "        \n",
    "        # Weighted sum\n",
    "        feats_V = tf.reduce_sum(feats_stack * attention_vectors, axis=1)\n",
    "        return feats_V\n",
    "\n",
    "def build_mthars_model(input_shape=(128, 9, 1), num_classes=6):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "\n",
    "    # --- Backbone Network ---\n",
    "    # Initial Conv Block\n",
    "    x = layers.Conv2D(64, (5, 1), strides=(3, 1), padding='same')(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "\n",
    "    # SKConv Blocks\n",
    "    x = SKConv(128)(x)\n",
    "    x = SKConv(256)(x)\n",
    "    \n",
    "    # Flatten/Dense (Transition to Recognition/Segmentation Network)\n",
    "    # The paper uses a \"Windows Generate\" and Conv1D structure here\n",
    "    # For a direct conversion of your snippet:\n",
    "    x_flatten = layers.Flatten()(x)\n",
    "    \n",
    "    # --- Multi-Task Heads (Based on Paper Figure 3) ---\n",
    "    \n",
    "    # 1. Recognition Head (Activity Category)\n",
    "    rec_head = layers.Dense(num_classes, name='recognition_output')(x_flatten)\n",
    "    rec_head = layers.LayerNormalization()(rec_head)\n",
    "    rec_output = layers.Activation('softmax')(rec_head)\n",
    "    \n",
    "    # 2. Segmentation Head (Activity vs Transition)\n",
    "    seg_head = layers.Dense(2, name='segmentation_output')(x_flatten)\n",
    "    seg_head = layers.LayerNormalization()(seg_head)\n",
    "    seg_output = layers.Activation('softmax')(seg_head)\n",
    "\n",
    "    model = models.Model(inputs=inputs, outputs=[rec_output, seg_output])\n",
    "    return model\n",
    "\n",
    "# Initialize and Compile\n",
    "model = build_mthars_model()\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss={\n",
    "        'recognition_output': 'categorical_crossentropy',\n",
    "        'segmentation_output': 'categorical_crossentropy'\n",
    "    },\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
